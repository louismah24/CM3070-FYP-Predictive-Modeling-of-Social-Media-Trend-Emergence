{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbec1750-a53d-4dd6-be9d-f9403702b17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YouTube Data Collection for Southeast Asian Food Travel Content\n",
    "# This notebook collects video data using YouTube Data API v3 and filters for Southeast Asian food travel content\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import os\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Install required packages (run once)\n",
    "# !pip install google-api-python-client pandas numpy requests\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "class YouTubeDataCollector:\n",
    "    def __init__(self, api_key: str):\n",
    "        \"\"\"\n",
    "        Initialize YouTube Data Collector\n",
    "        \n",
    "        Args:\n",
    "            api_key (str): YouTube Data API v3 key\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.youtube = build('youtube', 'v3', developerKey=api_key)\n",
    "        self.collected_videos = []\n",
    "        self.search_terms = [\n",
    "            # Thai cuisine\n",
    "            \"Thai street food\", \"Thai cooking\", \"Bangkok food\", \"Thai restaurant\",\n",
    "            \"Pad Thai\", \"Tom Yum\", \"Green curry Thai\", \"Thai market food\",\n",
    "            \n",
    "            # Vietnamese cuisine\n",
    "            \"Vietnamese pho\", \"Vietnamese street food\", \"Saigon food\", \"Hanoi food\",\n",
    "            \"Vietnamese cooking\", \"Banh mi\", \"Vietnamese restaurant\",\n",
    "            \n",
    "            # Malaysian cuisine\n",
    "            \"Malaysian food\", \"Kuala Lumpur food\", \"Malaysian hawker\", \"Nasi lemak\",\n",
    "            \"Malaysian street food\", \"Penang food\", \"Malaysian cooking\",\n",
    "            \n",
    "            # Singaporean cuisine\n",
    "            \"Singapore hawker\", \"Singapore food\", \"Singapore street food\",\n",
    "            \"Singapore cooking\", \"Hainanese chicken rice\",\n",
    "            \n",
    "            # Indonesian cuisine\n",
    "            \"Indonesian food\", \"Jakarta food\", \"Indonesian street food\",\n",
    "            \"Nasi goreng\", \"Indonesian cooking\", \"Bali food\",\n",
    "            \n",
    "            # Filipino cuisine\n",
    "            \"Filipino food\", \"Manila food\", \"Filipino cooking\", \"Adobo\",\n",
    "            \"Filipino street food\", \"Philippines food\",\n",
    "            \n",
    "            # General Southeast Asian terms\n",
    "            \"Southeast Asian cuisine\", \"ASEAN food\", \"Asian street food travel\",\n",
    "            \"Southeast Asian cooking\", \"Asian food travel\"\n",
    "        ]\n",
    "    \n",
    "    def search_videos(self, query: str, max_results: int = 50, order: str = 'relevance') -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Search for videos using YouTube Data API\n",
    "        \n",
    "        Args:\n",
    "            query (str): Search query\n",
    "            max_results (int): Maximum number of results per query\n",
    "            order (str): Order of results ('relevance', 'date', 'viewCount', 'rating')\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of video data dictionaries\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Calculate date range for recent content (last 2 years)\n",
    "            published_after = (datetime.now() - timedelta(days=730)).isoformat() + 'Z'\n",
    "            \n",
    "            search_response = self.youtube.search().list(\n",
    "                q=query,\n",
    "                part='id,snippet',\n",
    "                maxResults=max_results,\n",
    "                type='video',\n",
    "                order=order,\n",
    "                publishedAfter=published_after,\n",
    "                videoDuration='medium',  # 4-20 minutes\n",
    "                regionCode='SG'  # Singapore region for Southeast Asian content\n",
    "            ).execute()\n",
    "            \n",
    "            video_ids = [item['id']['videoId'] for item in search_response['items']]\n",
    "            \n",
    "            # Get detailed video statistics\n",
    "            videos_response = self.youtube.videos().list(\n",
    "                part='statistics,snippet,contentDetails',\n",
    "                id=','.join(video_ids)\n",
    "            ).execute()\n",
    "            \n",
    "            videos_data = []\n",
    "            for video in videos_response['items']:\n",
    "                video_data = self.extract_video_data(video)\n",
    "                if self.is_relevant_content(video_data):\n",
    "                    videos_data.append(video_data)\n",
    "            \n",
    "            print(f\"Found {len(videos_data)} relevant videos for query: '{query}'\")\n",
    "            return videos_data\n",
    "            \n",
    "        except HttpError as e:\n",
    "            print(f\"An HTTP error occurred: {e}\")\n",
    "            return []\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def extract_video_data(self, video: Dict) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract relevant data from YouTube video object\n",
    "        \n",
    "        Args:\n",
    "            video (Dict): YouTube video object from API\n",
    "        \n",
    "        Returns:\n",
    "            Dict: Processed video data\n",
    "        \"\"\"\n",
    "        snippet = video['snippet']\n",
    "        statistics = video['statistics']\n",
    "        content_details = video['contentDetails']\n",
    "        \n",
    "        # Parse duration from ISO 8601 format (PT4M13S -> 253 seconds)\n",
    "        duration_str = content_details['duration']\n",
    "        duration_seconds = self.parse_duration(duration_str)\n",
    "        \n",
    "        return {\n",
    "            'video_id': video['id'],\n",
    "            'title': snippet['title'],\n",
    "            'description': snippet['description'],\n",
    "            'channel_title': snippet['channelTitle'],\n",
    "            'channel_id': snippet['channelId'],\n",
    "            'published_at': snippet['publishedAt'],\n",
    "            'tags': snippet.get('tags', []),\n",
    "            'category_id': snippet['categoryId'],\n",
    "            'view_count': int(statistics.get('viewCount', 0)),\n",
    "            'like_count': int(statistics.get('likeCount', 0)),\n",
    "            'comment_count': int(statistics.get('commentCount', 0)),\n",
    "            'duration_seconds': duration_seconds,\n",
    "            'thumbnail_url': snippet['thumbnails']['high']['url'] if 'high' in snippet['thumbnails'] else '',\n",
    "            'language': snippet.get('defaultLanguage', 'unknown')\n",
    "        }\n",
    "    \n",
    "    def parse_duration(self, duration_str: str) -> int:\n",
    "        \"\"\"\n",
    "        Parse YouTube duration format (PT4M13S) to seconds\n",
    "        \n",
    "        Args:\n",
    "            duration_str (str): Duration in ISO 8601 format\n",
    "        \n",
    "        Returns:\n",
    "            int: Duration in seconds\n",
    "        \"\"\"\n",
    "        import re\n",
    "        pattern = r'PT(?:(\\d+)H)?(?:(\\d+)M)?(?:(\\d+)S)?'\n",
    "        match = re.match(pattern, duration_str)\n",
    "        \n",
    "        if not match:\n",
    "            return 0\n",
    "        \n",
    "        hours = int(match.group(1)) if match.group(1) else 0\n",
    "        minutes = int(match.group(2)) if match.group(2) else 0\n",
    "        seconds = int(match.group(3)) if match.group(3) else 0\n",
    "        \n",
    "        return hours * 3600 + minutes * 60 + seconds\n",
    "    \n",
    "    def is_relevant_content(self, video_data: Dict) -> bool:\n",
    "        \"\"\"\n",
    "        Filter videos to ensure they're relevant to Southeast Asian food travel\n",
    "        \n",
    "        Args:\n",
    "            video_data (Dict): Video data dictionary\n",
    "        \n",
    "        Returns:\n",
    "            bool: True if content is relevant\n",
    "        \"\"\"\n",
    "        # Combine title, description, and tags for content analysis\n",
    "        content_text = (\n",
    "            video_data['title'].lower() + ' ' + \n",
    "            video_data['description'].lower() + ' ' + \n",
    "            ' '.join(video_data['tags']).lower()\n",
    "        )\n",
    "        \n",
    "        # Southeast Asian countries and food-related keywords\n",
    "        sea_countries = [\n",
    "            'thailand', 'thai', 'vietnam', 'vietnamese', 'malaysia', 'malaysian',\n",
    "            'singapore', 'singaporean', 'indonesia', 'indonesian', 'philippines',\n",
    "            'filipino', 'myanmar', 'burma', 'cambodia', 'cambodian', 'laos',\n",
    "            'lao', 'brunei', 'bangkok', 'saigon', 'hanoi', 'kuala lumpur',\n",
    "            'penang', 'jakarta', 'bali', 'manila', 'phuket', 'ho chi minh'\n",
    "        ]\n",
    "        \n",
    "        food_keywords = [\n",
    "            'food', 'cuisine', 'cooking', 'recipe', 'restaurant', 'street food',\n",
    "            'hawker', 'market', 'eating', 'taste', 'flavor', 'dish', 'meal',\n",
    "            'cooking', 'chef', 'kitchen', 'spicy', 'noodles', 'rice', 'curry'\n",
    "        ]\n",
    "        \n",
    "        travel_keywords = [\n",
    "            'travel', 'trip', 'visit', 'tour', 'explore', 'journey', 'vacation',\n",
    "            'adventure', 'guide', 'vlog', 'experience'\n",
    "        ]\n",
    "        \n",
    "        # Check for Southeast Asian country mention\n",
    "        has_sea_country = any(country in content_text for country in sea_countries)\n",
    "        \n",
    "        # Check for food-related content\n",
    "        has_food_content = any(keyword in content_text for keyword in food_keywords)\n",
    "        \n",
    "        # Check for travel element (optional but preferred)\n",
    "        has_travel_element = any(keyword in content_text for keyword in travel_keywords)\n",
    "        \n",
    "        # Must have SEA country and food content\n",
    "        # Travel element is a bonus but not required\n",
    "        return has_sea_country and has_food_content\n",
    "    \n",
    "    def collect_comments(self, video_id: str, max_comments: int = 20) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Collect comments for a specific video\n",
    "        \n",
    "        Args:\n",
    "            video_id (str): YouTube video ID\n",
    "            max_comments (int): Maximum number of comments to collect\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: List of comment data\n",
    "        \"\"\"\n",
    "        try:\n",
    "            comments_response = self.youtube.commentThreads().list(\n",
    "                part='snippet',\n",
    "                videoId=video_id,\n",
    "                maxResults=max_comments,\n",
    "                order='relevance'\n",
    "            ).execute()\n",
    "            \n",
    "            comments = []\n",
    "            for item in comments_response['items']:\n",
    "                comment = item['snippet']['topLevelComment']['snippet']\n",
    "                comments.append({\n",
    "                    'video_id': video_id,\n",
    "                    'comment_text': comment['textDisplay'],\n",
    "                    'like_count': comment.get('likeCount', 0),\n",
    "                    'published_at': comment['publishedAt']\n",
    "                })\n",
    "            \n",
    "            return comments\n",
    "            \n",
    "        except HttpError as e:\n",
    "            print(f\"Could not retrieve comments for video {video_id}: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def collect_all_data(self, videos_per_query: int = 50, include_comments: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Collect data for all search terms and combine into single dataset\n",
    "        \n",
    "        Args:\n",
    "            videos_per_query (int): Number of videos to collect per search term\n",
    "            include_comments (bool): Whether to collect comments for sentiment analysis\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Complete dataset with comments aggregated per video\n",
    "        \"\"\"\n",
    "        all_videos = []\n",
    "        \n",
    "        print(f\"Starting data collection for {len(self.search_terms)} search terms...\")\n",
    "        \n",
    "        for i, term in enumerate(self.search_terms):\n",
    "            print(f\"\\nProcessing search term {i+1}/{len(self.search_terms)}: '{term}'\")\n",
    "            \n",
    "            # Collect videos for this search term\n",
    "            videos = self.search_videos(term, max_results=videos_per_query)\n",
    "            \n",
    "            # Collect comments if requested and aggregate them per video\n",
    "            if include_comments:\n",
    "                for video in videos:\n",
    "                    comments = self.collect_comments(video['video_id'])\n",
    "                    \n",
    "                    # Aggregate comment data for this video\n",
    "                    if comments:\n",
    "                        comment_texts = [c['comment_text'] for c in comments]\n",
    "                        comment_likes = [c['like_count'] for c in comments]\n",
    "                        \n",
    "                        video['comments_text'] = ' | '.join(comment_texts)  # Join with separator\n",
    "                        video['comments_count_collected'] = len(comments)\n",
    "                        video['comments_total_likes'] = sum(comment_likes)\n",
    "                        video['comments_avg_likes'] = np.mean(comment_likes) if comment_likes else 0\n",
    "                    else:\n",
    "                        video['comments_text'] = ''\n",
    "                        video['comments_count_collected'] = 0\n",
    "                        video['comments_total_likes'] = 0\n",
    "                        video['comments_avg_likes'] = 0\n",
    "                    \n",
    "                    time.sleep(0.1)  # Rate limiting\n",
    "            else:\n",
    "                # Add empty comment fields if not collecting comments\n",
    "                for video in videos:\n",
    "                    video['comments_text'] = ''\n",
    "                    video['comments_count_collected'] = 0\n",
    "                    video['comments_total_likes'] = 0\n",
    "                    video['comments_avg_likes'] = 0\n",
    "            \n",
    "            all_videos.extend(videos)\n",
    "            \n",
    "            # Rate limiting to respect API quotas\n",
    "            time.sleep(1)\n",
    "            \n",
    "            # Progress update\n",
    "            if (i + 1) % 5 == 0:\n",
    "                print(f\"Collected {len(all_videos)} videos so far...\")\n",
    "        \n",
    "        # Remove duplicates based on video_id\n",
    "        videos_df = pd.DataFrame(all_videos)\n",
    "        if not videos_df.empty:\n",
    "            videos_df = videos_df.drop_duplicates(subset=['video_id'])\n",
    "            print(f\"\\nTotal unique videos collected: {len(videos_df)}\")\n",
    "        \n",
    "        return videos_df\n",
    "    \n",
    "    def add_derived_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add derived features for analysis\n",
    "        \n",
    "        Args:\n",
    "            df (pd.DataFrame): Raw video data\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Enhanced dataset with derived features\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Convert published_at to datetime\n",
    "        df['published_at'] = pd.to_datetime(df['published_at'])\n",
    "        \n",
    "        # Extract time-based features\n",
    "        df['publish_hour'] = df['published_at'].dt.hour\n",
    "        df['publish_day_of_week'] = df['published_at'].dt.dayofweek\n",
    "        df['publish_month'] = df['published_at'].dt.month\n",
    "        \n",
    "        # Calculate engagement ratios\n",
    "        df['like_to_view_ratio'] = df['like_count'] / (df['view_count'] + 1)  # +1 to avoid division by zero\n",
    "        df['comment_to_view_ratio'] = df['comment_count'] / (df['view_count'] + 1)\n",
    "        df['engagement_score'] = (df['like_count'] + df['comment_count']) / (df['view_count'] + 1)\n",
    "        \n",
    "        # Duration categories\n",
    "        df['duration_category'] = pd.cut(df['duration_seconds'], \n",
    "                                       bins=[0, 300, 600, 1200, float('inf')], \n",
    "                                       labels=['Short', 'Medium', 'Long', 'Very Long'])\n",
    "        \n",
    "        # Title and description length\n",
    "        df['title_length'] = df['title'].str.len()\n",
    "        df['description_length'] = df['description'].str.len()\n",
    "        df['tags_count'] = df['tags'].apply(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "        \n",
    "        # Performance indicators (for labeling trending videos)\n",
    "        # Define trending based on view count percentiles within the dataset\n",
    "        view_threshold = df['view_count'].quantile(0.8)  # Top 20% by views\n",
    "        engagement_threshold = df['engagement_score'].quantile(0.75)  # Top 25% by engagement\n",
    "        \n",
    "        df['is_trending'] = (\n",
    "            (df['view_count'] >= view_threshold) & \n",
    "            (df['engagement_score'] >= engagement_threshold)\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3970f048-3a99-4306-8eca-e1091bfe134a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "API_KEY = \"xxx\"  # Replace with your actual API key\n",
    "VIDEOS_PER_QUERY = 50\n",
    "INCLUDE_COMMENTS = True\n",
    "OUTPUT_DIR = \"data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06bbf1fd-1827-42f4-8a52-4ba187deaec4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create output directory\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m os\u001b[38;5;241m.\u001b[39mmakedirs(OUTPUT_DIR, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize collector\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInitializing YouTube Data Collector...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Initialize collector\n",
    "print(\"Initializing YouTube Data Collector...\")\n",
    "collector = YouTubeDataCollector(API_KEY)\n",
    "\n",
    "# Collect data\n",
    "print(\"Starting data collection process...\")\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    videos_df = collector.collect_all_data(\n",
    "        videos_per_query=VIDEOS_PER_QUERY,\n",
    "        include_comments=INCLUDE_COMMENTS\n",
    "    )\n",
    "    \n",
    "    if not videos_df.empty:\n",
    "        # Add derived features\n",
    "        print(\"\\nAdding derived features...\")\n",
    "        videos_df = collector.add_derived_features(videos_df)\n",
    "        \n",
    "        # Data quality check\n",
    "        print(\"\\n=== DATA QUALITY SUMMARY ===\")\n",
    "        print(f\"Total videos collected: {len(videos_df)}\")\n",
    "        print(f\"Date range: {videos_df['published_at'].min()} to {videos_df['published_at'].max()}\")\n",
    "        print(f\"Average views: {videos_df['view_count'].mean():.0f}\")\n",
    "        print(f\"Videos marked as trending: {videos_df['is_trending'].sum()}\")\n",
    "        print(f\"Videos with comments: {(videos_df['comments_count_collected'] > 0).sum()}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(\"\\n=== SAMPLE VIDEO DATA ===\")\n",
    "        print(videos_df[['title', 'channel_title', 'view_count', 'like_count', 'comments_count_collected', 'is_trending']].head())\n",
    "        \n",
    "        # Save to single CSV file\n",
    "        output_filename = f\"{OUTPUT_DIR}/youtube_sea_food_travel_data.csv\"\n",
    "        videos_df.to_csv(output_filename, index=False, encoding='utf-8')\n",
    "        print(f\"\\nComplete dataset saved to: {output_filename}\")\n",
    "        \n",
    "        # Generate collection report\n",
    "        report = {\n",
    "            'collection_date': datetime.now().isoformat(),\n",
    "            'total_videos': len(videos_df),\n",
    "            'videos_with_comments': int((videos_df['comments_count_collected'] > 0).sum()),\n",
    "            'total_comments_collected': int(videos_df['comments_count_collected'].sum()),\n",
    "            'search_terms_used': len(collector.search_terms),\n",
    "            'date_range': {\n",
    "                'start': videos_df['published_at'].min().isoformat(),\n",
    "                'end': videos_df['published_at'].max().isoformat()\n",
    "            },\n",
    "            'trending_videos': int(videos_df['is_trending'].sum()),\n",
    "            'avg_views': float(videos_df['view_count'].mean()),\n",
    "            'avg_engagement_score': float(videos_df['engagement_score'].mean())\n",
    "        }\n",
    "        \n",
    "        report_filename = f\"{OUTPUT_DIR}/collection_report.json\"\n",
    "        with open(report_filename, 'w') as f:\n",
    "            json.dump(report, f, indent=2)\n",
    "        print(f\"Collection report saved to: {report_filename}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"No videos were collected. Please check your API key and search terms.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error during data collection: {e}\")\n",
    "\n",
    "finally:\n",
    "    end_time = time.time()\n",
    "    print(f\"\\nData collection completed in {end_time - start_time:.2f} seconds\")\n",
    "\n",
    "# Display collection statistics\n",
    "if 'videos_df' in locals() and not videos_df.empty:\n",
    "    print(\"\\n=== FINAL COLLECTION STATISTICS ===\")\n",
    "    print(f\"Unique channels: {videos_df['channel_id'].nunique()}\")\n",
    "    print(f\"Videos per channel (avg): {len(videos_df) / videos_df['channel_id'].nunique():.1f}\")\n",
    "    print(f\"View count distribution:\")\n",
    "    print(videos_df['view_count'].describe())\n",
    "    \n",
    "    print(f\"\\nTop 5 most viewed videos:\")\n",
    "    top_videos = videos_df.nlargest(5, 'view_count')[['title', 'channel_title', 'view_count', 'like_count']]\n",
    "    for idx, row in top_videos.iterrows():\n",
    "        print(f\"- {row['title'][:60]}... | {row['channel_title']} | {row['view_count']:,} views\")\n",
    "    \n",
    "    print(f\"\\nContent by country/cuisine (based on title keywords):\")\n",
    "    # Simple keyword counting in titles\n",
    "    countries = ['thai', 'vietnam', 'malaysia', 'singapore', 'indonesia', 'filipino']\n",
    "    for country in countries:\n",
    "        count = videos_df['title'].str.lower().str.contains(country).sum()\n",
    "        print(f\"- {country.capitalize()}: {count} videos\")\n",
    "    \n",
    "    print(f\"\\nComment collection summary:\")\n",
    "    print(f\"- Videos with comments collected: {(videos_df['comments_count_collected'] > 0).sum()}\")\n",
    "    print(f\"- Total comments collected: {videos_df['comments_count_collected'].sum()}\")\n",
    "    print(f\"- Average comments per video: {videos_df['comments_count_collected'].mean():.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517aa19f-1613-4184-9436-bbfcf123aefd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
